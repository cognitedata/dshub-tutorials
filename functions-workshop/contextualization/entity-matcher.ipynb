{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Matcher in production - contextualize time series to assets using Cognite Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every new customer, we ingest their data and perform some sort of contextualization by mapping time series to assets. Quite often, there is not an identical match between a field on the time series to e.g. the asset name, but a somewhat similar name. The most typical example we use in Cognite is the asset `21PT1019` and the time series `IAA_21PT1019.PV` or similar. In our contextualization toolbox (an [API](https://docs.cognite.com/api/playground/#operation/entityMatchingFit) with a corresponding [https://cognite-sdk-experimental.readthedocs-hosted.com/en/latest/cognite.html#contextualization](SDK)), we have a tool to solve this problem called entity matching. Entity matching means joining two datasets one a common key with fuzzyness, and our implementation is a machine learning model that can be trained supervised or unsupervised. \n",
    "\n",
    "In this tutorial, we'll take the time series and assets from the `publicdata` tenant, and deploy a Cognite Function that performs entity matching to map the time series to assets, and schedule it so it runs periodically. This can be used out of the box for many customers as an initial contextualization step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from cognite.experimental import CogniteClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = getpass()\n",
    "client = CogniteClient(\n",
    "    api_key=api_key,\n",
    "    project=\"functions-tutorial\",\n",
    "    client_name=\"DSHub\",\n",
    "    base_url=\"https://greenfield.cognitedata.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our function\n",
    "We now define the function we wish to deploy. In this simple example, we define the function within the notebook itself. This is convenient to get started quickly, but for larger functions, you would probably define the function in one or more separate files, or use the CI/CD in Github or Gitlab.\n",
    "\n",
    "Note that the function needs to be named `handle`, and the arguments to the function must be a subset of `(client, data, secrets)`. In this example we only need `client` and `data`. The `client` argument is an instance of a `CogniteClient`, which will be provided to us automatically when the function is deployed as long as we give an `api_key` when creating the function. The data we give when calling the function, is passed through the `data` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle(client, data):\n",
    "    # When deploying a function from a notebook like this, all imports must be performed inside the `handle` function.\n",
    "    from cognite.experimental import CogniteClient\n",
    "    from cognite.client.data_classes import TimeSeriesUpdate\n",
    "    import time\n",
    "    \n",
    "    # The entity matcher suggests matches with a certain score. To achieve a reasonable result, this score must be adjusted. \n",
    "    # The default value of 0.75 has been chosen by inspecting the outcome of this function, and may be different on data from other customers.\n",
    "    good_match_threshold = data.get(\"good_match_threshold\", 0.75)\n",
    "    \n",
    "    # Create experimental SDK client as the contextualization API's are in playground and are thus not available in the regular SDK.\n",
    "    client = CogniteClient(api_key = client.config.api_key, base_url = client.config.base_url, project = client.config.project)\n",
    "\n",
    "    # Download all assets and time series, using 5 requests in parallel\n",
    "    assets = client.assets.list(limit=-1, partitions=5)\n",
    "    time_series = client.time_series.list(limit=-1, partitions=5)\n",
    "    \n",
    "    # Create simplified objects with only name and id\n",
    "    assets_simplified = [{\"id\": asset.id, \"name\": asset.name} for asset in assets]\n",
    "    time_series_simplified = [{\"id\": ts.id, \"name\": ts.name} for ts in time_series]\n",
    "\n",
    "    # Train the ML Entity Matcher on the data. The SDK expects as input the array of objects you match FROM (time series) and a list of what you match TO (assets)\n",
    "    t0 = time.time()\n",
    "    model = client.entity_matching.fit_ml(match_from = time_series_simplified, match_to = assets_simplified)\n",
    "    print(f\"Training entity matcher model with id {model.model_id} ...\")\n",
    "    model.wait_for_completion()\n",
    "    t1 = time.time()\n",
    "    print(f\"Model {model.model_id} trained on {len(assets_simplified)} assets and {len(time_series_simplified)} time series using {t1-t0} seconds\")\n",
    "\n",
    "    # Use the ML Entity Matcher model to match the data. This model can be reused, so training is not necessary each time, but we do it for simplicity in this example.\n",
    "    t0 = time.time()\n",
    "    job = model.predict_ml(time_series_simplified)\n",
    "    result = job.result # This will wait for completion\n",
    "    t1 = time.time()\n",
    "    print(f\"Predict finished after {t1-t0} seconds on {len(time_series_simplified)} time series.\")\n",
    "    \n",
    "    # Filter out the best matches with the threshold specified in the input\n",
    "    good_match_count = 0\n",
    "    time_series_updates = []\n",
    "    for item in result[\"items\"]:\n",
    "        match_from = item[\"matchFrom\"] # Time series\n",
    "        matches = item[\"matches\"] # Suggested asset matches for the time series\n",
    "        good_matches = [match for match in matches if match[\"score\"] >= good_match_threshold]\n",
    "        if len(good_matches) > 0:\n",
    "            good_match_count += 1\n",
    "            best_match = good_matches[0]\n",
    "            time_series_updates.append(TimeSeriesUpdate(id=match_from[\"id\"]).asset_id.set(best_match[\"matchTo\"][\"id\"]))\n",
    "    \n",
    "    client.time_series.update(time_series_updates) # uncomment to actually update the asset_id field\n",
    "    print(f\"Matched {good_match_count} time series to assets\")\n",
    "    return {\n",
    "        \"matches\": good_match_count\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function by running it locally. We send in the `CogniteClient` and an empty dictionary as data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = handle(client, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying and calling the function\n",
    "Now that we have verified that it works, we can create a Cognite Function that performs this on demand or by a schedule. The function code is uploaded to CDF simply by setting the argument `function_handle` equal to `handle`, which is the name of the function we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = # Insert your name here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_id = f\"{name} entity matcher\"\n",
    "function = client.functions.create(\n",
    "    name = external_id, \n",
    "    external_id=external_id, \n",
    "    description = \"Entity matcher example\", \n",
    "    api_key = api_key, \n",
    "    function_handle = handle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this until it shows status ready\n",
    "function = client.functions.retrieve(external_id = external_id)\n",
    "function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the function is ready, we can call it. The default behaviour is that `function.call()` will wait until the function call is completed. You can also do `function.call(wait=False)` if you e.g. want to start multiple function calls at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_call = function.call() # This will wait for the function call to complete.\n",
    "function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the function failed, or you just want to see the logs, you can check the `function_call.get_logs()` function. All print statements and stack traces from errors are shown in the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_call.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you returned data from the function (like in the above example), you can access that from the `function_call.get_response()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_call.get_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a schedule\n",
    "We can schedule the function to call periodically by creating a `schedules` objects. It must point to a function with an external id, and the syntax for when the it is scheduled is unix cron (see https://crontab.guru to play around with values). Time zone for schedules is GMT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = client.functions.schedules.create(\n",
    "    name = f\"{name} entity matcher\", \n",
    "    function_external_id=external_id, \n",
    "    cron_expression = \"* * * * *\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "When we are done, we clean up after ourselves byt deleting the schedule and function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.functions.schedules.delete(id=schedule.id)\n",
    "client.functions.delete(id=function.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Entity Matching models with SDK-experimental demo\n",
    "\n",
    "This notebook uses time series and assets from the open industrial data tenant. It uses cognite-sdk-python-experimental to fit entity matcher models using different parameter combinations. The data is split into a training and test set. The models are compared using running time and the F1 score (combination of precision and recall) on the test set. To read the definition of precision and reacll see [here](https://en.wikipedia.org/wiki/Precision_and_recall). \n",
    "\n",
    "One model is chosen and the false positives (incorrect predicted matches by the model), and false negatives (matches the model were not able to find) are briefly investigated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get access to CDF\n",
    "We assume you have some basic knowledge of CDF and the SDK. If not, please follow the 'lab' tutorials first.\n",
    "\n",
    "For this tutorial you need access to the publicdata project / tenant. If you don't have one, you can get an API-key [here](https://content.cognite.com/open-industrial-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules\n",
    "We need to import some Python modules in order to interact with CDF. We will use the Python SDK with Experimental Extensions, which we below refer to as a client. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from cognite.experimental import CogniteClient\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a client\n",
    "\n",
    "When you create the CogniteClient below, getpass will ask for your API key in an extra password field. Simply paste ypu publicdata API-key and press shift+enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"publicdata\"\n",
    "api_key = getpass(\"Please enter Open Industrial Data API-KEY: \")\n",
    "client = CogniteClient(project=project, api_key=api_key, client_name=\"dshub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data\n",
    "\n",
    "In this tutorial we will use time series and assets data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = client.time_series.list(limit=-1)\n",
    "assets = client.assets.list(limit=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train and test data\n",
    "\n",
    "Shuffle the time-series list randomly and use the first 60% as the training set and the rest as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.6\n",
    "num_train_samples = round(len(time_series) * train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1006)\n",
    "random.shuffle(time_series)\n",
    "time_series_train = time_series[0:num_train_samples]\n",
    "time_series_test = time_series[num_train_samples:]\n",
    "\n",
    "true_matches_train = [(x[\"id\"], x[\"asset_id\"]) for x in time_series_train.dump()]\n",
    "true_matches_test = [(x[\"id\"], x[\"asset_id\"]) for x in time_series_test.dump()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different parameter combinations\n",
    "\n",
    "In this tutorial 10 parameter combinations are tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_combs = [\n",
    "    {\n",
    "        \"name\": \"simple\",\n",
    "        \"keys_from_to\": [(\"name\", \"name\")],\n",
    "        \"feature_type\": \"simple\",\n",
    "        \"classifier\": \"RandomForest\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"simple_extra_keys\",\n",
    "        \"keys_from_to\": [(\"name\", \"name\"), (\"description\", \"description\")],\n",
    "        \"feature_type\": \"simple\",\n",
    "        \"classifier\": \"RandomForest\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"bigram\",\n",
    "        \"keys_from_to\": [(\"name\", \"name\")],\n",
    "        \"feature_type\": \"bigram\",\n",
    "        \"classifier\": \"RandomForest\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"bigram_extra_keys\",\n",
    "        \"keys_from_to\": [(\"name\", \"name\"), (\"description\", \"description\")],\n",
    "        \"feature_type\": \"bigram\",\n",
    "        \"classifier\": \"RandomForest\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"simple_all_keys\",\n",
    "        \"keys_from_to\": [\n",
    "            (\"name\", \"name\"),\n",
    "            (\"description\", \"description\"),\n",
    "            (\"name\", \"description\"),\n",
    "            (\"description\", \"name\"),\n",
    "        ],\n",
    "        \"feature_type\": \"simple\",\n",
    "        \"classifier\": \"RandomForest\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"frequency_weighted_bigram\",\n",
    "        \"keys_from_to\": [(\"name\", \"name\")],\n",
    "        \"feature_type\": \"simple\",\n",
    "        \"classifier\": \"RandomForest\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"frequency_weighted_bigram\",\n",
    "        \"keys_from_to\": [(\"name\", \"name\")],\n",
    "        \"feature_type\": \"frequency-weighted-bigram\",\n",
    "        \"classifier\": \"RandomForest\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"bigram_extra_tokenizers\",\n",
    "        \"keys_from_to\": [(\"name\", \"name\")],\n",
    "        \"feature_type\": \"bigram-extra-tokenizers\",\n",
    "        \"classifier\": \"RandomForest\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"bigram_combo\",\n",
    "        \"keys_from_to\": [(\"name\", \"name\")],\n",
    "        \"feature_type\": \"bigram-combo\",\n",
    "        \"classifier\": \"RandomForest\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"unsupervised\",\n",
    "        \"keys_from_to\": [(\"name\", \"name\")],\n",
    "        \"feature_type\": \"bigram\",\n",
    "        \"classifier\": \"Unsupervised\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_matches, predicted_matches, score_threshold):\n",
    "    \"\"\" \n",
    "    Evaluate the performance of a model. \n",
    "    Find true-postives, false-negative and false-positives gien a score threshold.\n",
    "    Calculate the precision, recall and F1 score.\n",
    "    \"\"\"\n",
    "    predicted_positive_match = [\n",
    "        x\n",
    "        for x in predicted_matches\n",
    "        if x.get(\"matches\") and x[\"matches\"][0][\"score\"] > score_threshold\n",
    "    ]\n",
    "    predicted_positive_matches = [\n",
    "        (x[\"matchFrom\"][\"id\"], x[\"matches\"][0][\"matchTo\"][\"id\"])\n",
    "        for x in predicted_positive_match\n",
    "    ]\n",
    "\n",
    "    # Calculate precision, recall and f1_beta\n",
    "    true_positives = [x for x in predicted_positive_matches if x in true_matches]\n",
    "    false_postives = [x for x in predicted_positive_matches if x not in true_matches]\n",
    "    false_negatives = [x for x in true_matches if x not in predicted_positive_matches]\n",
    "\n",
    "    precision = len(true_positives) / (len(true_positives) + len(false_postives))\n",
    "    recall = len(true_positives) / (len(true_positives) + len(false_negatives))\n",
    "\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return (true_positives, false_postives, false_negatives, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through all test combinations. Fit a model using the training data and predict time-series to asset matches for the test data. Use the true matches for the test data to evaluate the performance of the model.\n",
    "Store run time as the time from the model starts training until it is done predicting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = 0.75\n",
    "\n",
    "for parameter_comb in test_combs:\n",
    "    if parameter_comb[\"classifier\"] == \"Unsupervised\":\n",
    "        true_matches = None\n",
    "        classifier = None\n",
    "    else:\n",
    "        true_matches = true_matches_train\n",
    "        classifier = parameter_comb[\"classifier\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = client.entity_matching.fit(\n",
    "        match_from=time_series_train,\n",
    "        match_to=assets,\n",
    "        true_matches=true_matches_train,\n",
    "        keys_from_to=parameter_comb[\"keys_from_to\"],\n",
    "        feature_type=parameter_comb[\"feature_type\"],\n",
    "        classifier=classifier,\n",
    "        complete_missing=True,\n",
    "        name=parameter_comb[\"name\"],\n",
    "    )\n",
    "    results = model.predict(\n",
    "        match_from=time_series_test, match_to=assets, complete_missing=True\n",
    "    ).result\n",
    "    parameter_comb[\"runtime\"] = time.time() - start_time\n",
    "    (tp, fp, fn, precision, recall, f1) = evaluate(\n",
    "        true_matches=true_matches_test,\n",
    "        predicted_matches=results[\"items\"],\n",
    "        score_threshold=score_threshold,\n",
    "    )\n",
    "    parameter_comb[\"precision\"], parameter_comb[\"recall\"], parameter_comb[\"f1\"] = (\n",
    "        precision,\n",
    "        recall,\n",
    "        f1,\n",
    "    )\n",
    "    parameter_comb[\"model_id\"] = model.model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [test_comb[\"f1\"] for test_comb in test_combs]\n",
    "y = [test_comb[\"runtime\"] for test_comb in test_combs]\n",
    "labels = [test_comb[\"name\"] for test_comb in test_combs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.scatter(x, y)\n",
    "\n",
    "ax.set_ylabel(\"Run time (s)\")\n",
    "ax.set_xlabel(\"F1 (performance)\")\n",
    "ax.set_title(\"Compare parameter combinations\")\n",
    "\n",
    "for i, txt in enumerate(labels):\n",
    "    ax.annotate(txt, (x[i], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a model, retrieve and update it and investigate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model by name\n",
    "model_id_selected = [\n",
    "    x[\"model_id\"] for x in test_combs if x[\"name\"] == \"bigram_extra_tokenizers\"\n",
    "][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrive the model\n",
    "model = client.entity_matching.retrieve(id=model_id_selected)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the description of the model\n",
    "Updating the model is not yet available in the SDK. It will soon be, but for now we have to use the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.description = \"My chosen model\"\n",
    "model = client.entity_matching.update(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test-data and evaluate ones more.\n",
    "results = model.predict(match_from=time_series_test, match_to=assets).result\n",
    "(tp, fp, fn, precision, recall, f1) = evaluate(\n",
    "    true_matches=true_matches_test,\n",
    "    predicted_matches=results[\"items\"],\n",
    "    score_threshold=score_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicted matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_matches_list = []\n",
    "for x in results[\"items\"]:\n",
    "    time_series_id = x[\"matchFrom\"][\"id\"]\n",
    "    time_series_name = x[\"matchFrom\"][\"name\"]\n",
    "    matches = x[\"matches\"]\n",
    "    if matches:\n",
    "        score = matches[0][\"score\"]\n",
    "        asset_name = matches[0][\"matchTo\"][\"name\"]\n",
    "    else:\n",
    "        score, asset_name = \"\", \"\"\n",
    "    predicted_matches_list.append(\n",
    "        {\n",
    "            \"time_series_id\": time_series_id,\n",
    "            \"time_series_name\": time_series_name,\n",
    "            \"predicted_asset_name\": asset_name,\n",
    "            \"score\": score,\n",
    "        }\n",
    "    )\n",
    "df_predicted_matches = pd.DataFrame(predicted_matches_list)\n",
    "df_predicted_matches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series = time_series_test.to_pandas()[[\"id\", \"name\", \"assetId\"]].add_prefix(\n",
    "    \"time_series_\"\n",
    ")\n",
    "df_assets = assets.to_pandas()[[\"id\", \"name\"]]\n",
    "\n",
    "false_negatives = pd.merge(\n",
    "    pd.DataFrame(fn), df_time_series, how=\"left\", left_on=0, right_on=\"time_series_id\"\n",
    ")\n",
    "false_negatives = pd.merge(\n",
    "    false_negatives,\n",
    "    df_assets.add_prefix(\"asset_\"),\n",
    "    how=\"left\",\n",
    "    left_on=1,\n",
    "    right_on=\"asset_id\",\n",
    ")\n",
    "false_negatives = pd.merge(false_negatives, df_predicted_matches, how=\"left\")\n",
    "false_negatives.drop(\n",
    "    [0, 1, \"time_series_id\", \"time_series_assetId\", \"asset_id\"], axis=1, inplace=True\n",
    ")\n",
    "false_negatives.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete all created models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id_list = [x[\"model_id\"] for x in test_combs]\n",
    "client.entity_matching.delete(model_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
